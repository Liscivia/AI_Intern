# Notion â†’ AI â†’ Google Sheets Automation Pipeline

> End-to-end Python workflow that turns a **Completed** Due-Diligence Questionnaire (DDQ) in Notion into a fully-scored entry in our research Google Sheet â€“ no servers, no databases, just APIs and scheduled jobs.

---

## âœ¨ What you get

| Step | Output |
|------|--------|
| 1. Watcher | Detects cards in a Notion CRM whose **Completed** checkbox flipped to â˜‘ï¸ in DDQs |
| 2. Deep Research | Institutional-grade Markdown "Deep" Research Report written by an LLM (define the depth of the web research in `src/research.py`)|
| 3. Report Writer | Child page **"AI Deep Research Report"** created under the relative Notion card |
| 4. Scoring Agent *(WIP)* | Strict JSON with 20+ scoring columns |
| 5. Sheet Pusher *(WIP)* | Values & rationales filled into our Google Sheet |

All activity is logged to `logs/` so the whole flow is fully auditable.

---

## ğŸ—ºï¸ Repository Layout

```text
.
â”œâ”€â”€ src/                 # All runtime code
â”‚   â”œâ”€â”€ watcher.py       # 1ï¸âƒ£ Poll Notion for completed DDQs
â”‚   â”œâ”€â”€ research.py      # 2ï¸âƒ£ Deep-research wrapper around `deep-research-py`
â”‚   â”œâ”€â”€ writer.py        # 3ï¸âƒ£ Convert Markdown â†’ Notion blocks & publish
â”‚   â”œâ”€â”€ scorer.py        # 4ï¸âƒ£ Score DDQ + report (TBD)
â”‚   â””â”€â”€ pusher.py        # 5ï¸âƒ£ Write scores to Google Sheets (TBD)
â”œâ”€â”€ main.py              # Orchestrator (weekly cron)
â”œâ”€â”€ requirements.txt     # Python deps
â””â”€â”€ tests/               # Pytest suite
```

---

## ğŸš€ Quick Start

1. **Clone & enter** the repo
   ```bash
   git clone https://github.com/Liscivia/AI_intern.git
   cd AI_intern
   ```
2. **Create a virtual-env** (Python â‰¥3.11)
   ```bash
   python -m venv .venv
   source .venv/bin/activate   # Windows: .venv\Scripts\activate
   ```
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   playwright install  # only once, downloads browser binaries
   ```
4. **Configure secrets** â€“ copy the template below into `.env` and fill the values:
   ```bash
   cp .env.example .env  # if the file exists; otherwise create it manually
   ```
   ```dotenv
   # .env
   NOTION_TOKEN=secret_xxxx
   NOTION_DB_ID=xxx
   OPENAI_API_KEY=sk-...
   OPENAI_MODEL=o3
   FIRECRAWL_KEY=fc_... # advised to utilize Firecrawl to prod
   GOOGLE_SERVICE_JSON=/absolute/path/to/google-service.json
   ```
5. **Run the pipeline once** (with tests, until `main.py` is implemented):

   `pytest tests/test_multiple.py -rs -s` to produce all Deep Research Reports for all projects with completed DDQs within the previous week in the Notion db, and publish them in their cards.
   `pytest tests/test_writer.py -rs -s` to produce a single Deep Research Report for the last completed DDQ in the Notion db and publish it on the card.
   `tests/test_research.py -s` to produce a single Deep Research Report for the last completed DDQ in the Notion db and be able to read it in `reports/`
   ...

---

## ğŸ—“ï¸ Scheduled Execution

Add a weekly cron (or GitHub Actions, Cloud Run, etc.) that simply calls:

---

## ğŸ“œ Logging

| File | Purpose |
|------|---------|
| `logs/watcher.log` | Notion polling (step 1) |
| `logs/research.log` | Deep-research orchestration |
| `logs/writer.log` | Markdown â†’ Notion publishing |

Each entry is a single `key=value` line so you can `grep` it easily.

---

## ğŸª„ Common Issues

| Symptom | Fix |
|---------|-----|
| `RuntimeError: Environment variable NOTION_TOKEN is required.` | Ensure `.env` is loaded (`source .venv/bin/activate` then `pip install python-dotenv` or export vars in your shell). |
| Browser launch fails in CI | Use Playwright's `--browser chromium --with-deps` docker image or run `playwright install chromium` during build. |
| `firecrawl` 429 errors | The library is set as default scraper; tune the rate-limit envs or switch to Playwright by un-commenting the fallback in `src/research.py`. |

---


